{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfeebac-fba5-487a-96d6-5c21484e847f",
   "metadata": {},
   "source": [
    "# co\n",
    "\n",
    "> Covariance and Coherence Matrix Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9eaed-c4c4-498b-b280-c15946104223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cli/co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdef397-9ce4-4d64-958f-e4bb98392daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542f477-0d58-4d9e-8f85-67562a31774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import zarr\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import colorcet\n",
    "\n",
    "import dask\n",
    "from dask import array as da\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "from decorrelation.co import emperical_co_pc\n",
    "from decorrelation.cli.utils.logging import get_logger, log_args\n",
    "from decorrelation.cli.utils.dask import pad_internal, get_cuda_cluster, get_pc_chunk_size\n",
    "\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c776c-a947-47cb-b61f-7129e8ac4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "@log_args\n",
    "def de_emperical_co_pc(rslc:str, # input: rslc stack\n",
    "                       ds_can_is_shp:str, # input: bool array indicating the SHPs of every pixel\n",
    "                       ds_can_idx:str, # input: bool array indicating DS candidate\n",
    "                       ds_can_coh:str, # output: complex coherence matrix for DS candidate\n",
    "                       az_chunk_size:int=None, # azimuth chunk size, optional. Default: the azimuth chunk size in rslc stack\n",
    "                       n_pc_chunk:int=None, # number of point chunk, optional.\n",
    "                       pc_chunk_size:int=None, # chunk size of output zarr dataset, optional. Default: same as ds_can_is_shp\n",
    "                       ds_can_coh_ave_fig:str=None, # path to the plot of average coherence matrix of DS candidate, optional. Default: no plot\n",
    "                       log=None, # log file. Default: no log file\n",
    "                       ):\n",
    "    '''estimate emperical coherence matrix on point cloud data.\n",
    "    Only one of `n_pc_chunk` and `pc_chunk_size` needs to be setted. The other one is automatically determined.\n",
    "    If all of them are not setted, the `n_pc_chunk` will be setted as the number of azimuth chunks.\n",
    "    '''\n",
    "    rslc_path = rslc\n",
    "    ds_can_is_shp_path = ds_can_is_shp\n",
    "    ds_can_idx_path = ds_can_idx\n",
    "    ds_can_coh_path = ds_can_coh\n",
    "    logger = get_logger(logfile=log)\n",
    "\n",
    "    rslc_zarr = zarr.open(rslc_path,mode='r')\n",
    "    logger.zarr_info(rslc_path, rslc_zarr)\n",
    "    assert rslc_zarr.ndim == 3, \"rslc dimentation is not 3.\"\n",
    "\n",
    "    ds_can_is_shp_zarr = zarr.open(ds_can_is_shp_path,mode='r')\n",
    "    logger.zarr_info(ds_can_is_shp_path, ds_can_is_shp_zarr)\n",
    "    assert ds_can_is_shp_zarr.ndim == 3, \"ds_can_is_shp dimentation is not 3.\"\n",
    "\n",
    "    ds_can_idx_zarr = zarr.open(ds_can_idx_path,mode='r')\n",
    "    logger.zarr_info(ds_can_idx_path, ds_can_idx_zarr)\n",
    "    assert ds_can_idx_zarr.ndim == 2, \"ds_can_idx dimentation is not 2.\"\n",
    "    logger.info('loading ds_can_idx into memory.')\n",
    "    ds_can_idx = zarr.open(ds_can_idx_path,mode='r')[:]\n",
    "\n",
    "    az_win, r_win = ds_can_is_shp_zarr.shape[1:]\n",
    "    az_half_win = int((az_win-1)/2)\n",
    "    r_half_win = int((r_win-1)/2)\n",
    "    logger.info(f'got azimuth window size and half azimuth window size from is_shp shape: {az_win}, {az_half_win}')\n",
    "    logger.info(f'got range window size and half range window size from is_shp shape: {r_win}, {r_half_win}')\n",
    "\n",
    "    if not az_chunk_size:\n",
    "        az_chunk_size = rslc_zarr.chunks[0]\n",
    "        logger.info('using default parallel processing azimuth chunk size from rslc dataset.')\n",
    "    logger.info('parallel processing azimuth chunk size: '+str(az_chunk_size))\n",
    "\n",
    "    logger.info('starting dask CUDA local cluster.')\n",
    "    cluster, client = get_cuda_cluster()\n",
    "    logger.info('dask local CUDA cluster started.')\n",
    "\n",
    "    logger.info('create raster bool array is_ds_can from ds_can_idx')\n",
    "    np_is_ds_can = np.zeros(rslc_zarr.shape[:2],dtype=bool)\n",
    "    np_is_ds_can[(ds_can_idx[0],ds_can_idx[1])] = True\n",
    "    logger.info('create dask bool array is_ds_can')\n",
    "    cpu_is_ds_can = da.from_array(np_is_ds_can,chunks=(az_chunk_size,-1))\n",
    "    logger.darr_info('is_ds_can', cpu_is_ds_can)\n",
    "    \n",
    "    logger.info('Using azimuth chunk size as the processing chunk size.')\n",
    "    logger.info('Calculate point chunk size')\n",
    "    cpu_ds_can_idx = da.nonzero(cpu_is_ds_can)\n",
    "    idx_0 = cpu_ds_can_idx[0]\n",
    "    process_pc_chunk_size = idx_0.compute_chunk_sizes().chunks[0]\n",
    "    logger.info(f'Point chunk size: {process_pc_chunk_size}')\n",
    "\n",
    "    cpu_rslc = da.from_zarr(rslc_path,chunks=(az_chunk_size,*rslc_zarr.shape[1:]))\n",
    "    logger.darr_info('rslc', cpu_rslc)\n",
    "\n",
    "    cpu_ds_can_is_shp = da.from_zarr(ds_can_is_shp_path,chunks=(process_pc_chunk_size,*ds_can_is_shp_zarr.shape[1:]))\n",
    "    logger.darr_info('ds_can_is_shp', cpu_ds_can_is_shp)\n",
    "\n",
    "    depth = {0:az_half_win, 1:r_half_win, 2:0}; boundary = {0:'none',1:'none',2:'none'}\n",
    "    cpu_rslc_overlap = da.overlap.overlap(cpu_rslc,depth=depth, boundary=boundary)\n",
    "    logger.info('setting shared boundaries between rlsc chunks.')\n",
    "    logger.darr_info('rslc_overlap', cpu_rslc_overlap)\n",
    "\n",
    "    depth = {0:az_half_win, 1:r_half_win}\n",
    "    cpu_is_ds_can_padded = pad_internal(cpu_is_ds_can,depth=depth)\n",
    "    logger.info('padding zero between is_ds_can chunks.')\n",
    "    logger.darr_info('is_ds_can_padded', cpu_is_ds_can_padded)\n",
    "\n",
    "    logger.info(f'estimating coherence matrix.')\n",
    "    rslc_overlap = cpu_rslc_overlap.map_blocks(cp.asarray)\n",
    "    is_ds_can_padded = cpu_is_ds_can_padded.map_blocks(cp.asarray)\n",
    "    ds_can_is_shp = cpu_ds_can_is_shp.map_blocks(cp.asarray)\n",
    "    \n",
    "    emperical_co_pc_delayed = delayed(emperical_co_pc,pure=True,nout=2)\n",
    "\n",
    "    is_ds_can_padded_delayed = is_ds_can_padded.to_delayed()\n",
    "    is_ds_can_padded_delayed = np.squeeze(is_ds_can_padded_delayed,axis=-1)\n",
    "    rslc_overlap_delayed = rslc_overlap.to_delayed()\n",
    "    rslc_overlap_delayed = np.squeeze(rslc_overlap_delayed,axis=(-2,-1))\n",
    "    ds_can_is_shp_delayed = ds_can_is_shp.to_delayed()\n",
    "    ds_can_is_shp_delayed = np.squeeze(ds_can_is_shp_delayed,axis=(-2,-1))\n",
    "\n",
    "    ds_can_coh_delayed = np.empty_like(rslc_overlap_delayed,dtype=object)\n",
    "    ds_can_idx_delayed = np.empty_like(rslc_overlap_delayed,dtype=object)\n",
    "\n",
    "    nimage = rslc_overlap.shape[-1]\n",
    "    with np.nditer(rslc_overlap_delayed,flags=['multi_index','refs_ok'], op_flags=['readwrite']) as it:\n",
    "        for block in it:\n",
    "            idx = it.multi_index\n",
    "            ds_can_idx_delayed[idx] = delayed(cp.where)(is_ds_can_padded_delayed[idx])\n",
    "            ds_can_coh_delayed[idx] = emperical_co_pc_delayed(rslc_overlap_delayed[idx],ds_can_idx_delayed[idx],ds_can_is_shp_delayed[idx])[1]\n",
    "            chunk_shape = (ds_can_is_shp.blocks[idx].shape[0],nimage,nimage)\n",
    "            dtype = rslc_overlap.dtype\n",
    "            ds_can_coh_delayed[idx] = da.from_delayed(ds_can_coh_delayed[idx],shape=chunk_shape,meta=cp.array((),dtype=dtype))\n",
    "\n",
    "    ds_can_coh = da.block(ds_can_coh_delayed.reshape(*ds_can_coh_delayed.shape,1,1).tolist())\n",
    "    cpu_ds_can_coh = ds_can_coh.map_blocks(cp.asnumpy)\n",
    "    logger.info(f'got coherence matrix.')\n",
    "\n",
    "    # zarr do not support irregular chunk size\n",
    "    pc_chunk_size = get_pc_chunk_size(az_chunk_size,ds_can_coh.shape[0],pc_chunk_size=pc_chunk_size,n_pc_chunk=n_pc_chunk,logger=logger)\n",
    "    cpu_ds_can_coh = cpu_ds_can_coh.rechunk((pc_chunk_size,nimage,nimage))\n",
    "    logger.info('rechunking ds_can_coh to chunk size (for saving with zarr): '+str(cpu_ds_can_coh.chunksize))\n",
    "\n",
    "    logger.info('saving ds_can_coh.')\n",
    "    _cpu_ds_can_coh = cpu_ds_can_coh.to_zarr(ds_can_coh_path,overwrite=True,compute=False)\n",
    "\n",
    "    cpu_ds_can_coh_ave = da.abs(cpu_ds_can_coh).mean(axis=0)\n",
    "\n",
    "    logger.info('computing graph setted. doing all the computing.')\n",
    "    #This function is really slow just because the coherence is very big and rechunk and saving takes too much time.\n",
    "    # I do not find any solution to it.\n",
    "    cpu_ds_can_coh_ave_result = da.compute(_cpu_ds_can_coh,cpu_ds_can_coh_ave)[1]\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    logger.info('computing finished.')\n",
    "    cluster.close()\n",
    "    logger.info('dask cluster closed.')\n",
    "\n",
    "    if ds_can_coh_ave_fig:\n",
    "        logger.info('plotting average coherence matrix.')\n",
    "        fig, ax = plt.subplots(1,1,figsize=(15,10))\n",
    "        pcm = ax.imshow(cpu_ds_can_coh_ave_result,cmap=colorcet.cm.fire)\n",
    "        ax.set(title='Average Coherence Matrix',xlabel='Image Index',ylabel='Image Index')\n",
    "        fig.colorbar(pcm)\n",
    "        fig.show()\n",
    "        fig.savefig(ds_can_coh_ave_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b423d4e-80f4-4b45-9eb7-32d5f637710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslc = './raw/rslc.zarr'\n",
    "ds_can_is_shp = './shp/ds_can_is_shp.zarr'\n",
    "ds_can_idx = './shp/ds_can_idx.zarr'\n",
    "ds_can_coh = './co/ds_can_coh.zarr'\n",
    "az_chunk_size = 1000\n",
    "pc_chunk_size = None\n",
    "log = './co/co.log'\n",
    "ds_can_coh_ave_fig = './co/ds_can_coh_ave.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da0bef-09bf-4ad3-ad13-4047f53a0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - fetching args:\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - rslc = './raw/rslc.zarr'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ds_can_is_shp = './shp/ds_can_is_shp.zarr'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ds_can_idx = './shp/ds_can_idx.zarr'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ds_can_coh = './co/ds_can_coh.zarr'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - az_chunk_size = None\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - n_pc_chunk = None\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - pc_chunk_size = None\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ds_can_coh_ave_fig = './co/ds_can_coh_ave.png'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - log = './co/co.log'\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - fetching args done.\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./raw/rslc.zarr zarray shape: (2500, 1834, 17)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./raw/rslc.zarr zarray chunks: (1000, 1834, 1)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./raw/rslc.zarr zarray dtype: complex64\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_is_shp.zarr zarray shape: (740397, 11, 11)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_is_shp.zarr zarray chunks: (1000, 11, 11)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_is_shp.zarr zarray dtype: bool\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_idx.zarr zarray shape: (2, 740397)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_idx.zarr zarray chunks: (2, 1000)\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - ./shp/ds_can_idx.zarr zarray dtype: int64\n",
      "2023-10-13 20:49:21 - de_emperical_co_pc - INFO - loading ds_can_idx into memory.\n",
      "2023-10-13 20:49:22 - de_emperical_co_pc - INFO - got azimuth window size and half azimuth window size from is_shp shape: 11, 5\n",
      "2023-10-13 20:49:22 - de_emperical_co_pc - INFO - got range window size and half range window size from is_shp shape: 11, 5\n",
      "2023-10-13 20:49:22 - de_emperical_co_pc - INFO - using default parallel processing azimuth chunk size from rslc dataset.\n",
      "2023-10-13 20:49:22 - de_emperical_co_pc - INFO - parallel processing azimuth chunk size: 1000\n",
      "2023-10-13 20:49:22 - de_emperical_co_pc - INFO - starting dask CUDA local cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-10-13 20:49:24,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - dask local CUDA cluster started.\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - create raster bool array is_ds_can from ds_can_idx\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - create dask bool array is_ds_can\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can dask array shape: (2500, 1834)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can dask array chunksize: (1000, 1834)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can dask array dtype: bool\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - Using azimuth chunk size as the processing chunk size.\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - Calculate point chunk size\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - Point chunk size: (346329, 274921, 119147)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc dask array shape: (2500, 1834, 17)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc dask array chunksize: (1000, 1834, 17)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc dask array dtype: complex64\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - ds_can_is_shp dask array shape: (740397, 11, 11)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - ds_can_is_shp dask array chunksize: (346329, 11, 11)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - ds_can_is_shp dask array dtype: bool\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - setting shared boundaries between rlsc chunks.\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc_overlap dask array shape: (2520, 1834, 17)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc_overlap dask array chunksize: (1010, 1834, 17)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - rslc_overlap dask array dtype: complex64\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - padding zero between is_ds_can chunks.\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can_padded dask array shape: (2520, 1834)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can_padded dask array chunksize: (1010, 1834)\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - is_ds_can_padded dask array dtype: bool\n",
      "2023-10-13 20:49:30 - de_emperical_co_pc - INFO - estimating coherence matrix.\n",
      "2023-10-13 20:49:31 - de_emperical_co_pc - INFO - got coherence matrix.\n",
      "2023-10-13 20:49:31 - de_emperical_co_pc - INFO - set pc_chunk_size as az_chunk_size: 1000\n",
      "2023-10-13 20:49:31 - de_emperical_co_pc - INFO - rechunking ds_can_coh to chunk size (for saving with zarr): (1000, 17, 17)\n",
      "2023-10-13 20:49:31 - de_emperical_co_pc - INFO - saving ds_can_coh.\n",
      "2023-10-13 20:49:31 - de_emperical_co_pc - INFO - computing graph setted. doing all the computing.\n",
      "2023-10-13 20:50:00 - de_emperical_co_pc - INFO - computing finished.\n",
      "2023-10-13 20:50:01 - de_emperical_co_pc - INFO - dask cluster closed.\n",
      "CPU times: user 3.56 s, sys: 2.67 s, total: 6.23 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "de_emperical_co_pc(rslc,ds_can_is_shp,ds_can_idx,ds_can_coh,log=log,ds_can_coh_ave_fig=ds_can_coh_ave_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90411255-8598-4cd7-b889-1a4706dde6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: de_emperical_co_pc [-h] [--az_chunk_size AZ_CHUNK_SIZE]\n",
      "                          [--n_pc_chunk N_PC_CHUNK]\n",
      "                          [--pc_chunk_size PC_CHUNK_SIZE]\n",
      "                          [--ds_can_coh_ave_fig DS_CAN_COH_AVE_FIG] [--log LOG]\n",
      "                          rslc ds_can_is_shp ds_can_idx ds_can_coh\n",
      "\n",
      "estimate emperical coherence matrix on point cloud data. Only one of\n",
      "`n_pc_chunk` and `pc_chunk_size` needs to be setted. The other one is\n",
      "automatically determined. If all of them are not setted, the `n_pc_chunk` will\n",
      "be setted as the number of azimuth chunks.\n",
      "\n",
      "positional arguments:\n",
      "  rslc                                  input: rslc stack\n",
      "  ds_can_is_shp                         input: bool array indicating the SHPs of\n",
      "                                        every pixel\n",
      "  ds_can_idx                            input: bool array indicating DS\n",
      "                                        candidate\n",
      "  ds_can_coh                            output: complex coherence matrix for DS\n",
      "                                        candidate\n",
      "\n",
      "options:\n",
      "  -h, --help                            show this help message and exit\n",
      "  --az_chunk_size AZ_CHUNK_SIZE         azimuth chunk size, optional. Default:\n",
      "                                        the azimuth chunk size in rslc stack\n",
      "  --n_pc_chunk N_PC_CHUNK               number of point chunk, optional.\n",
      "  --pc_chunk_size PC_CHUNK_SIZE         chunk size of output zarr dataset,\n",
      "                                        optional. Default: same as ds_can_is_shp\n",
      "  --ds_can_coh_ave_fig DS_CAN_COH_AVE_FIG\n",
      "                                        path to the plot of average coherence\n",
      "                                        matrix of DS candidate, optional.\n",
      "                                        Default: no plot\n",
      "  --log LOG                             log file. Default: no log file\n"
     ]
    }
   ],
   "source": [
    "!de_emperical_co_pc -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ed875-cada-4cf9-b895-f47984fb8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.83 s, sys: 3.56 s, total: 7.39 s\n",
      "Wall time: 5.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#| hide\n",
    "import numpy as np\n",
    "import zarr\n",
    "import cupy as cp\n",
    "from decorrelation.shp import ks_test\n",
    "rslc_zarr = zarr.open('./raw/rslc.zarr',mode='r')\n",
    "ds_can_is_shp_zarr = zarr.open('./shp/ds_can_is_shp.zarr','r')\n",
    "ds_can_idx_zarr = zarr.open('./shp/ds_can_idx.zarr','r')\n",
    "\n",
    "rslc_cpu = rslc_zarr[:]; rslc = cp.asarray(rslc_cpu)\n",
    "ds_can_is_shp_cpu = ds_can_is_shp_zarr[:]; ds_can_is_shp = cp.asarray(ds_can_is_shp_cpu)\n",
    "ds_can_idx_cpu = ds_can_idx_zarr[:]; ds_can_idx = cp.asarray(ds_can_idx_cpu)\n",
    "\n",
    "ds_can_coh = emperical_co_pc(rslc,ds_can_idx,ds_can_is_shp)[1]\n",
    "\n",
    "ds_can_coh_cpu = cp.asnumpy(ds_can_coh)\n",
    "nimage = rslc.shape[-1]\n",
    "ds_can_chunk_size = math.ceil(ds_can_coh.shape[0]/3)\n",
    "test_ds_can_coh_zarr = zarr.open('./co/test_ds_can_coh.zarr','w',shape=ds_can_coh_cpu.shape,chunks=(ds_can_chunk_size,nimage,nimage),dtype=ds_can_coh_cpu.dtype)\n",
    "test_ds_can_coh_zarr[:] = ds_can_coh_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741e85f-1d25-42ca-a1b9-03cc76705bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ds_can_coh_result = zarr.open('./co/ds_can_coh.zarr','r')[:]\n",
    "ds_can_coh_test = zarr.open('./co/test_ds_can_coh.zarr','r')[:]\n",
    "np.testing.assert_array_equal(ds_can_coh_cpu,ds_can_coh_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e745d5-90a7-4ed1-a149-7bfdba7a5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aa92d-a076-4d5e-bb1b-2f6ce279d60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work] *",
   "language": "python",
   "name": "conda-env-work-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
